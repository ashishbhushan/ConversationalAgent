// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!
//
// Protofile syntax: PROTO3

package ServerDIR.grpc.llm_service


object LLMServiceGrpc {
  val METHOD_STREAM_CONVERSATION: _root_.io.grpc.MethodDescriptor[ServerDIR.grpc.llm_service.ConversationRequest, ServerDIR.grpc.llm_service.ConversationResponse] =
    _root_.io.grpc.MethodDescriptor.newBuilder()
      .setType(_root_.io.grpc.MethodDescriptor.MethodType.UNARY)
      .setFullMethodName(_root_.io.grpc.MethodDescriptor.generateFullMethodName("ServerDIR.LLMService", "StreamConversation"))
      .setSampledToLocalTracing(true)
      .setRequestMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[ServerDIR.grpc.llm_service.ConversationRequest])
      .setResponseMarshaller(_root_.scalapb.grpc.Marshaller.forMessage[ServerDIR.grpc.llm_service.ConversationResponse])
      .setSchemaDescriptor(_root_.scalapb.grpc.ConcreteProtoMethodDescriptorSupplier.fromMethodDescriptor(ServerDIR.grpc.llm_service.LlmServiceProto.javaDescriptor.getServices().get(0).getMethods().get(0)))
      .build()
  
  val SERVICE: _root_.io.grpc.ServiceDescriptor =
    _root_.io.grpc.ServiceDescriptor.newBuilder("ServerDIR.LLMService")
      .setSchemaDescriptor(new _root_.scalapb.grpc.ConcreteProtoFileDescriptorSupplier(ServerDIR.grpc.llm_service.LlmServiceProto.javaDescriptor))
      .addMethod(METHOD_STREAM_CONVERSATION)
      .build()
  
  trait LLMService extends _root_.scalapb.grpc.AbstractService {
    override def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[LLMService] = LLMService
    def streamConversation(request: ServerDIR.grpc.llm_service.ConversationRequest): scala.concurrent.Future[ServerDIR.grpc.llm_service.ConversationResponse]
  }
  
  object LLMService extends _root_.scalapb.grpc.ServiceCompanion[LLMService] {
    implicit def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[LLMService] = this
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.ServiceDescriptor = ServerDIR.grpc.llm_service.LlmServiceProto.javaDescriptor.getServices().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.ServiceDescriptor = ServerDIR.grpc.llm_service.LlmServiceProto.scalaDescriptor.services(0)
    def bindService(serviceImpl: LLMService, executionContext: scala.concurrent.ExecutionContext): _root_.io.grpc.ServerServiceDefinition =
      _root_.io.grpc.ServerServiceDefinition.builder(SERVICE)
      .addMethod(
        METHOD_STREAM_CONVERSATION,
        _root_.io.grpc.stub.ServerCalls.asyncUnaryCall((request: ServerDIR.grpc.llm_service.ConversationRequest, observer: _root_.io.grpc.stub.StreamObserver[ServerDIR.grpc.llm_service.ConversationResponse]) => {
          serviceImpl.streamConversation(request).onComplete(scalapb.grpc.Grpc.completeObserver(observer))(
            executionContext)
        }))
      .build()
  }
  
  trait LLMServiceBlockingClient {
    def serviceCompanion: _root_.scalapb.grpc.ServiceCompanion[LLMService] = LLMService
    def streamConversation(request: ServerDIR.grpc.llm_service.ConversationRequest): ServerDIR.grpc.llm_service.ConversationResponse
  }
  
  class LLMServiceBlockingStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions = _root_.io.grpc.CallOptions.DEFAULT) extends _root_.io.grpc.stub.AbstractStub[LLMServiceBlockingStub](channel, options) with LLMServiceBlockingClient {
    override def streamConversation(request: ServerDIR.grpc.llm_service.ConversationRequest): ServerDIR.grpc.llm_service.ConversationResponse = {
      _root_.scalapb.grpc.ClientCalls.blockingUnaryCall(channel, METHOD_STREAM_CONVERSATION, options, request)
    }
    
    override def build(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): LLMServiceBlockingStub = new LLMServiceBlockingStub(channel, options)
  }
  
  class LLMServiceStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions = _root_.io.grpc.CallOptions.DEFAULT) extends _root_.io.grpc.stub.AbstractStub[LLMServiceStub](channel, options) with LLMService {
    override def streamConversation(request: ServerDIR.grpc.llm_service.ConversationRequest): scala.concurrent.Future[ServerDIR.grpc.llm_service.ConversationResponse] = {
      _root_.scalapb.grpc.ClientCalls.asyncUnaryCall(channel, METHOD_STREAM_CONVERSATION, options, request)
    }
    
    override def build(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): LLMServiceStub = new LLMServiceStub(channel, options)
  }
  
  object LLMServiceStub extends _root_.io.grpc.stub.AbstractStub.StubFactory[LLMServiceStub] {
    override def newStub(channel: _root_.io.grpc.Channel, options: _root_.io.grpc.CallOptions): LLMServiceStub = new LLMServiceStub(channel, options)
    
    implicit val stubFactory: _root_.io.grpc.stub.AbstractStub.StubFactory[LLMServiceStub] = this
  }
  
  def bindService(serviceImpl: LLMService, executionContext: scala.concurrent.ExecutionContext): _root_.io.grpc.ServerServiceDefinition = LLMService.bindService(serviceImpl, executionContext)
  
  def blockingStub(channel: _root_.io.grpc.Channel): LLMServiceBlockingStub = new LLMServiceBlockingStub(channel)
  
  def stub(channel: _root_.io.grpc.Channel): LLMServiceStub = new LLMServiceStub(channel)
  
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.ServiceDescriptor = ServerDIR.grpc.llm_service.LlmServiceProto.javaDescriptor.getServices().get(0)
  
}